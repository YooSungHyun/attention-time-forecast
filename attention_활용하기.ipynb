{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본인의 DATA SET load는 여기서 알아서 진행하도록 하자.\n",
    "=="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder/Decoder train / test set 작성.\n",
    "---\n",
    "#### 현재 소스에서는 Decoder에서 Y값을 이동시키며 교사강요 시키는 소스가 아니므로, Y data set도 있어야함\n",
    "\n",
    "#### 예시 \n",
    "enc_train.shape : (batch_size, time_step, feature_len) <br />\n",
    "dec_train.shape : (batch_size, time_step, wanted input decoder data len)  <br />\n",
    "y_train.shape : (batch_size, time_step, predict data len)  <br />\n",
    " <br />\n",
    "enc_test.shape : (batch_size, time_step, feature_len)  <br />\n",
    "dec_test.shape : (batch_size, time_step, wanted input decoder data len)  <br />\n",
    "y_test.shape : (batch_size, time_step, predict data len)  <br />\n",
    "\n",
    "총 6개의 변수가 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=1000\n",
    "steps_per_epoch = len(enc_train)\n",
    "buffer_size = len(enc_train)\n",
    "\n",
    "train_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (enc_train, dec_train, y_train)\n",
    "    )\n",
    "    .batch(batch_size)\n",
    "#     .shuffle(buffer_size=20000)\n",
    ")\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (enc_test, dec_test, y_test)\n",
    ").batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from attention import *\n",
    "\n",
    "# 0:내적어텐션 / 1:바다나우어텐션\n",
    "attention_cate = 0\n",
    "predict_len = 1\n",
    "n_hidden = 128\n",
    "\n",
    "'''\n",
    "내 생각에 아직 구체적으로 테스트해보진 못했지만, loss 계산 부분이 뭔가 매 시간 loss를 계산한다던지 했으면,\n",
    "좀 더 정확도 좋은 예측값을 얻어낼 수 있지 않을까도 싶다.(현재는 모든 time-step loss 계산후 한꺼번에 적용)\n",
    "각 epoch당 loss가 효과적으로 변화되지 못하지 않을까? 하는 의문도 있다.\n",
    "'''\n",
    "\n",
    "if attention_cate == 0:\n",
    "    encoder = attention_enc(n_hidden)\n",
    "    decoder = dot_product_dec(n_hidden)\n",
    "    \n",
    "    enc_x, dec_x, dec_y = next(iter(train_ds))\n",
    "    print(enc_x.shape, dec_x.shape, dec_y.shape)\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(inputs,labels,loss_fn,optimizer):\n",
    "        loss = 0\n",
    "        encoder_input, decoder_input = inputs\n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden, enc_c = encoder(encoder_input)\n",
    "            \n",
    "            # 교사강요를 위한 for문\n",
    "            for t in range(0, decoder_input.shape[1]):\n",
    "                # decoder의 time-step을 1개로 분리\n",
    "                dec_input = tf.reshape(decoder_input[:,t],(-1,1,decoder_input.shape[2]))\n",
    "                # 레퍼런스대로 enc값과 dec값을 이용하여 학습 시작\n",
    "                predictions, attention_weights = decoder(dec_input, enc_output, enc_hidden, enc_c)\n",
    "                # 교사강요때문에, 각 time-step에 대한 label을 predictions값과 바로바로 비교하여 로스를 더한다.\n",
    "                loss += loss_fn(tf.reshape(labels[:,t],(-1,1,1)), predictions)\n",
    "        batch_loss = tf.reduce_mean(loss)\n",
    "        variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "        # 전부다 더해진 loss를 통하여 gradients를 조정하게된다.\n",
    "        gradients = tape.gradient(loss,variables)\n",
    "        optimizer.apply_gradients(zip(gradients,variables))\n",
    "\n",
    "        return batch_loss\n",
    "\n",
    "    loss_fn = keras.losses.mean_absolute_error\n",
    "    optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "    \n",
    "    import time\n",
    "    \n",
    "    for epoch in range(5000):\n",
    "        start = time.time()\n",
    "        total_loss = 0\n",
    "\n",
    "        for (batch, (enc, dec, label)) in enumerate(train_ds.take(steps_per_epoch)):\n",
    "            inputs = [enc,dec]\n",
    "            batch_loss = train_step(inputs,label,loss_fn,optimizer)\n",
    "            total_loss += batch_loss\n",
    "            if batch % 100 == 0:\n",
    "                print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,batch, batch_loss.numpy()))\n",
    "\n",
    "        print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss.numpy() / steps_per_epoch))\n",
    "        print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "        \n",
    "elif attention_cate == 1:\n",
    "    encoder = attention_enc(n_hidden)\n",
    "    decoder = bahdanau_dec(n_hidden, 'v')\n",
    "    \n",
    "    enc_x, dec_x, dec_y = next(iter(train_ds))\n",
    "    print(enc_x.shape, dec_x.shape, dec_y.shape)\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(inputs,labels,loss_fn,optimizer):\n",
    "        loss = None\n",
    "        encoder_input, decoder_input = inputs\n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden, enc_c = encoder(encoder_input)\n",
    "            # 바다나우 어텐션은 t-1을 활용하는 성질때문에, 가장 처음은 enc_hidden을, 이후에는 dec_hidden을 사용해야해서\n",
    "            # 전단에 이런 작업들이 발생한다.\n",
    "            dec_hidden = enc_hidden\n",
    "            dec_input = tf.expand_dims(decoder_input[:,0], 1)\n",
    "            \n",
    "            # range는 데이터 특성을 보고 0부터 시작할지, 1부터 시작할지 잘 정하도록 하자.\n",
    "            for t in range(0, labels.shape[1]):\n",
    "                predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_output)\n",
    "                dec_hidden = dec_hidden[0]\n",
    "                \n",
    "                # loss로 개선할 수 있지 않을까 싶어서, loss를 단순 다 더하는 형태에서\n",
    "                # 각 time-step 별로 loss를 배열화 해서 던져보기로 했다. 필요에 따라 바꿔서 사용하길 바란다.\n",
    "#                 loss += loss_fn(labels[:,t], predictions)\n",
    "                if loss == None:\n",
    "                    loss = tf.expand_dims(loss_fn(labels[:,t], predictions),1)\n",
    "                    print(loss.shape)\n",
    "                else:\n",
    "                    loss = tf.concat([loss,tf.expand_dims(loss_fn(labels[:,t], predictions),1)], axis=-1)\n",
    "                print(\"after_transpose_shape : \", loss.shape)\n",
    "#                 tf.print(\"loss : \",loss, summarize=-1, output_stream=sys.stdout)\n",
    "                dec_input = tf.expand_dims(labels[:,t], 1)\n",
    "        \n",
    "        batch_loss = tf.reduce_mean(loss)\n",
    "        variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "        # 배열로 쌓아서 time-step만큼의 loss를 던져도 계산은 잘된다.\n",
    "        gradients = tape.gradient(loss,variables)\n",
    "        optimizer.apply_gradients(zip(gradients,variables))\n",
    "\n",
    "        return batch_loss\n",
    "\n",
    "    loss_fn = keras.losses.MAE\n",
    "    optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "    import time\n",
    "    for epoch in range(10000):\n",
    "        start = time.time()\n",
    "        total_loss = 0\n",
    "\n",
    "        for (batch, (enc, dec, label)) in enumerate(train_ds.take(steps_per_epoch)):\n",
    "            inputs = [enc,dec]\n",
    "            batch_loss = train_step(inputs,label,loss_fn,optimizer)\n",
    "            total_loss += batch_loss\n",
    "            if batch % 100 == 0:\n",
    "                print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,batch, batch_loss.numpy()))\n",
    "\n",
    "        print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss.numpy() / steps_per_epoch))\n",
    "        print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 바다나우를 사용한 경우 해당 evaluate를 사용해 test 진행 가능하다.\n",
    "def bada_evaluate (enc_x_test, dec_x_test, lookup):\n",
    "    if lookup == 'h':\n",
    "        plot_shape = (dec_x_test.shape[1], enc_x_test.shape[2])\n",
    "    elif lookup == 'v':\n",
    "        plot_shape = (dec_x_test.shape[1], enc_x_test.shape[1])\n",
    "    attention_plot = np.zeros(plot_shape)\n",
    "    enc_out, enc_hidden, _ = encoder(enc_x_test)\n",
    "    dec_hidden = enc_hidden\n",
    "    pred_result = []\n",
    "    \n",
    "    # 바다나우 특성상 0번째는 수동으로 한번 돌아가고,\n",
    "    dec_input = tf.reshape(dec_x_test[:,0],(-1,1,dec_x_test.shape[2]))\n",
    "    predictions, dec_hidden, attention_weights  = decoder(dec_input, dec_hidden, enc_out)\n",
    "    pred_result.append(predictions)\n",
    "    # weights도 0번째는 한번 저장한다.\n",
    "    # attention_weights가 한 시점에 대한 lstm_output_feature만큼 나오게되니까, mean으로 간단하게 출력해보기로 했다.\n",
    "    # 참고 : https://www.tensorflow.org/tutorials/text/nmt_with_attention?hl=ko\n",
    "    attention_weights = tf.reduce_mean(attention_weights, axis=0)\n",
    "    attention_plot[0] = attention_weights.numpy().squeeze()\n",
    "    \n",
    "    # decoder의 최종 time-step까지 돌면서 예측을 진행한다.\n",
    "    # 이부분은 참고할 것이 학습을 decoder의 다음값을 교사강요 시키지 않고, 어떤 1:1매칭의 예측을 해야된다면, 다른 형태로 구현되어야 한다.\n",
    "    for t in range(1, dec_x_test.shape[1]):\n",
    "        # enc_output를 디코더에 전달합니다.\n",
    "        print(predictions.numpy().shape)\n",
    "        dec_hidden = dec_hidden[0]\n",
    "        dec_input = tf.expand_dims(predictions, 1)\n",
    "#         dec_input = tf.reshape(dec_x_test[:,t],(-1,1,dec_x_test.shape[2]))\n",
    "#         dec_input = np.concatenate((dec_x_test[:,t,:-1].reshape(-1,1,dec_x_test[:,t,:-1].shape[1]),predictions.numpy()),axis=2)\n",
    "        predictions, dec_hidden, attention_weights  = decoder(dec_input, dec_hidden, enc_out)\n",
    "        attention_weights = tf.reduce_mean(attention_weights, axis=0)\n",
    "        attention_plot[t] = attention_weights.numpy().squeeze()\n",
    "        pred_result.append(predictions)\n",
    "    return np.array(pred_result), attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이런식으로 0번째 time-step에 대한 예측을 진행해볼 수도 있음.\n",
    "enc_out, enc_hidden, _ = encoder(enc_test)\n",
    "dec_hidden = enc_hidden\n",
    "dec_input = tf.reshape(dec_test[:,0],(-1,1,dec_test.shape[2]))\n",
    "\n",
    "predictions, dec_hidden, attention_weights  = decoder(dec_input, dec_hidden, enc_out)\n",
    "print(predictions.numpy().shape)\n",
    "dec_input = tf.expand_dims(predictions, 1)\n",
    "dec_hidden = dec_hidden[0]\n",
    "predictions, dec_hidden, attention_weights  = decoder(dec_input, dec_hidden, enc_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측값 출력\n",
    "tf.print(predictions[0], output_stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 내적을 사용한 경우 해당 evaluate를 사용해 test 진행 가능하다.\n",
    "def evaluate (enc_x_test, dec_x_test, lookup):\n",
    "    attention_plot = np.zeros(plot_shape)\n",
    "    enc_out, enc_hidden, enc_c = encoder(enc_x_test)\n",
    "    pred_result = []\n",
    "    \n",
    "    for t in range(0, dec_x_test.shape[1]):\n",
    "        # decoder에서 feature라고 생각되는 값을 넣어서 다음값을 예측하게되는 형태로도 구현하려고 concat을 써서, 강제로 예측값을 제외한\n",
    "        # 다른 특징값들을 합치는 작업들도 진행을 했었다.\n",
    "        # 단순하게 값만 예측하는 형태라면 그냥 decoder의 t-1값을 가지고 t를 예측하도록, 학습자가 가르친 형태로 evaluate를 구현하면 된다.\n",
    "        if t==0:\n",
    "            dec_input = tf.reshape(dec_x_test[:,t],(-1,1,dec_x_test.shape[2]))\n",
    "        else:\n",
    "            dec_input = np.concatenate((dec_x_test[:,t,:-1].reshape(-1,1,dec_x_test[:,t,:-1].shape[1]),predictions.numpy()),axis=2)\n",
    "        predictions, attention_weights  = decoder(dec_input, enc_out, enc_hidden, enc_c)\n",
    "        attention_weights = tf.reduce_mean(attention_weights, axis=0)\n",
    "        attention_plot[t] = attention_weights.numpy().squeeze()\n",
    "        pred_result.append(predictions)\n",
    "    return np.array(pred_result), attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, att_w = evaluate(enc_test,dec_test,'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (100,100)\n",
    "plt.rcParams['axes.grid'] = True\n",
    "\n",
    "# heatmap by plt.pcolor()\n",
    "# xticks와 yticks는 본인들의 encoder decoder time-step을 고려하여 숫자를 집어넣도록 하자.\n",
    "plt.pcolor(att_w)\n",
    "plt.xticks(np.arange(0, 196, 1), list(range(0,196)), fontsize=100)\n",
    "plt.yticks(np.arange(0, 98, 1), list(range(0,98)), fontsize=100)\n",
    "plt.title('Heatmap by plt.pcolor()', fontsize=20)\n",
    "plt.xlabel('Input_TimeSequence', fontsize=100)\n",
    "plt.ylabel('Input_History_TimeSquence', fontsize=100)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "623px",
    "left": "1550px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
