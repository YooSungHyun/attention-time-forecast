{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본인의 DATA SET load는 여기서 알아서 진행하도록 하자.\n",
    "=="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder/Decoder train / test set 작성.\n",
    "---\n",
    "#### 현재 소스에서는 Decoder에서 Y값을 이동시키며 교사강요 시키는 소스가 아니므로, Y data set도 있어야함\n",
    "\n",
    "#### 예시 \n",
    "enc_train.shape : (batch_size, time_step, feature_len) <br />\n",
    "dec_train.shape : (batch_size, time_step, wanted input decoder data len)  <br />\n",
    "y_train.shape : (batch_size, time_step, predict data len)  <br />\n",
    " <br />\n",
    "enc_test.shape : (batch_size, time_step, feature_len)  <br />\n",
    "dec_test.shape : (batch_size, time_step, wanted input decoder data len)  <br />\n",
    "y_test.shape : (batch_size, time_step, predict data len)  <br />\n",
    "\n",
    "총 6개의 변수가 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from DARNN import DARNN\n",
    "\n",
    "# time series\n",
    "T = 24\n",
    "\n",
    "# encoder hidden state cnt\n",
    "m = 128\n",
    "\n",
    "# decoder hidden state cnt\n",
    "p = 128\n",
    "\n",
    "batch_size = 2000\n",
    "\n",
    "# 논문에서 encoder, decoder hidden state cnt는 서로 같아야 성능이 제일 좋다고 한다 (64, 128일때가 논문상 베스트)\n",
    "model = DARNN(T=T, m=m, p=p, target_len=98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch 돌아가는 쪽 소스를 수정할 거면, 굳이, tf dataset을 사용할 필요는 없다.\n",
    "\n",
    "train_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices(\n",
    "        (enc_train, dec_train, y_train)\n",
    "    )\n",
    "    .batch(batch_size)\n",
    "#     .shuffle(buffer_size=20000)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (enc_test, dec_test, y_test)\n",
    ").batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(model,inputs,labels,loss_fn,optimizer,train_loss):\n",
    "    with tf.GradientTape() as tape:\n",
    "        prediction = model(inputs,training=True)\n",
    "        print(prediction.shape)\n",
    "        print(labels.shape)\n",
    "        loss = loss_fn(labels,prediction)\n",
    "        print(loss)\n",
    "    gradients = tape.gradient(loss,model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients,model.trainable_variables))\n",
    "    train_loss(loss)\n",
    "\n",
    "loss_fn = tf.keras.losses.MAE\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "train_loss = tf.keras.metrics.Mean(name = 'train_loss')\n",
    "\n",
    "for epoch in range(3000):\n",
    "    for enc,dec,label in train_ds:\n",
    "        inputs = [enc,dec]\n",
    "        train_step(model,inputs,label,loss_fn,optimizer,train_loss)\n",
    "\n",
    "    print(f\"epoch : {epoch+1}, train_loss : {train_loss.result()}\")\n",
    "    train_loss.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = tf.keras.metrics.Mean(name = 'test_loss')\n",
    "\n",
    "@tf.function\n",
    "def test_step(model,inputs,labels,loss_fn,test_loss):\n",
    "    prediction = model(inputs,training=True)\n",
    "    loss = loss_fn(labels,prediction)\n",
    "    test_loss(loss)\n",
    "    return prediction\n",
    "\n",
    "i=0\n",
    "for enc,dec,label in test_ds:\n",
    "    inputs = [enc,dec]\n",
    "    pred = test_step(model,inputs,label,loss_fn,test_loss)\n",
    "    if i==0:\n",
    "        preds = pred.numpy()\n",
    "        labels = label.numpy()\n",
    "        i+=1\n",
    "    else:\n",
    "        preds = np.concatenate([preds, pred.numpy()],axis=0)\n",
    "        labels = np.concatenate([labels,label.numpy()],axis=0)\n",
    "\n",
    "print(test_loss.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.font_manager as fm\n",
    "from matplotlib import rc\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beta는 temporal attention에서 나온 시간에 대한 attention weight를 시각화할 수 있다.\n",
    "\n",
    "enc,dec,label = next(iter(test_ds))\n",
    "inputs = [enc, dec]\n",
    "pred = model(inputs)\n",
    "\n",
    "beta = []\n",
    "\n",
    "# for문은 time step 만큼 돌린다.\n",
    "for i in range(24):\n",
    "    beta.append(np.mean(model.decoder.beta_t[:,i,0].numpy()))  # batch, T, 1\n",
    "\n",
    "plt.bar(x = range(24), height=beta, color = 'orange')\n",
    "plt.title(\"Beta\")\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"prob\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha는 Input Attention에서 나온 특징들에 대한 attention weight를 시각화 할 수 있다.\n",
    "\n",
    "# variable_key는 특징값들이 들어가있으면된다.\n",
    "variable_key = list(variable_dict.keys())\n",
    "alpha = []\n",
    "variables = []\n",
    "\n",
    "# for문은 feature len만큼 돌린다.\n",
    "for i in range(5):\n",
    "    alpha.append(np.mean(model.encoder.alpha_t[:,0,i].numpy()))\n",
    "    for key in variabel_key:\n",
    "        # 같은 의미의 값이 여러개 있는경우 숫자를 붙혀줄 수 있는데, 특징이 유니크하게 1개인경우 해당 소스는 필요없다.\n",
    "        if f\"{i}\" in variable_dict[key]:\n",
    "            variables.append(f\"{key}{i}\")\n",
    "\n",
    "plt.figure(figsize = (12,12))\n",
    "plt.bar(x=variables,height=alpha, color = 'orange')\n",
    "plt.title(\"alpha\")\n",
    "plt.xlabel(\"variables\")\n",
    "plt.xticks(rotation = 90)\n",
    "plt.ylabel(\"prob\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "674px",
    "left": "1063px",
    "right": "20px",
    "top": "136px",
    "width": "692px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
